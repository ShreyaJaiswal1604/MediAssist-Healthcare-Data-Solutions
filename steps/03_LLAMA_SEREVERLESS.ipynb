{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "SERVERLESS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " meta-llama/Llama-3.1-8B-Instruct\n",
    " \n",
    " https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from huggingface_hub import InferenceClient\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_USER = \"DOLPHIN\"\n",
    "SNOWFLAKE_PASSWORD = \"Maapaa@1603\"\n",
    "SNOWFLAKE_ACCOUNT = \"URB63596\"\n",
    "SNOWFLAKE_WAREHOUSE = \"ANIMAL_TASK_WH\"\n",
    "SNOWFLAKE_DATABASE = \"mimic_iv_medi_assist\"\n",
    "SNOWFLAKE_SCHEMA = \"staging_mimc\"\n",
    "DISCHARGE_TABLE = \"MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, TEXT \n",
    "FROM MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\n",
    "WHERE TEXT IS NOT NULL\n",
    "LIMIT 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Connecting to Snowflake...\n",
      "[INFO] Executing query to fetch clinical notes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/v5dnvjg12ws3p1zszn_fvgt80000gn/T/ipykernel_20749/3369770645.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  clinical_notes_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] printing dataframe data...\n",
      "\n",
      "   SUBJECT_ID   HADM_ID                                               TEXT\n",
      "0    15992303  22502053   \\nName:  ___               Unit No:   ___\\n \\...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] Connecting to Snowflake...\")\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    "    warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "    database=SNOWFLAKE_DATABASE,\n",
    "    schema=SNOWFLAKE_SCHEMA,)\n",
    "\n",
    "print(\"[INFO] Executing query to fetch clinical notes...\\n\")\n",
    "clinical_notes_df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n[INFO] printing dataframe data...\\n\")\n",
    "print(clinical_notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HF_API_KEY = \"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\"\n",
    "HF_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Initializing Hugging Face client...\n",
      "\n",
      "\n",
      "[INFO] Step 2: Processing clinical notes...\n",
      "====================================================================================\n",
      "\n",
      "[INFO] Processing SUBJECT_ID: 15992303 \n",
      "[INFO] Processing HADM_ID: 22502053\n",
      "\n",
      "[INFO] Formatting message for the model...\n",
      "\n",
      "[INFO] Sending request to the Hugging Face model Llama-3.1-8B-Instruct.........\n",
      "\n",
      " [INFO] completion...\n",
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='[\"K76.0\", \"K72.9\", \"E90.0\", \"R65.2\", \"B05.9\"]', tool_calls=None), logprobs=None)], created=1733214887, id='', model='meta-llama/Llama-3.1-8B-Instruct', system_fingerprint='2.3.1-sha-a094729', usage=ChatCompletionOutputUsage(completion_tokens=31, prompt_tokens=5192, total_tokens=5223))\n",
      "\n",
      " [INFO] response...\n",
      "[\"K76.0\", \"K72.9\", \"E90.0\", \"R65.2\", \"B05.9\"]\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#HF_API_KEY = \"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\"\n",
    "HF_API_KEY = \"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\"\n",
    "\n",
    "# Initialize the Hugging Face Inference Client\n",
    "print(\"\\n[INFO] Initializing Hugging Face client...\\n\")\n",
    "client = InferenceClient(api_key=HF_API_KEY)\n",
    "\n",
    "print(\"\\n[INFO] Step 2: Processing clinical notes...\")\n",
    "\n",
    "for _, row in clinical_notes_df.iterrows():\n",
    "    print(\"====================================================================================\")\n",
    "    subject_id = row[\"SUBJECT_ID\"]\n",
    "    hadm_id = row[\"HADM_ID\"]\n",
    "    clinical_note = row[\"TEXT\"]\n",
    "\n",
    "    print(f\"\\n[INFO] Processing SUBJECT_ID: {subject_id} \\n[INFO] Processing HADM_ID: {hadm_id}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Formatting message for the model...\")\n",
    "\n",
    "    #TOP 10 ICD CODES\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "            Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes as a Python list of strings. No explanation and code required in the output.\n",
    "\n",
    "            Clinical Note:\n",
    "            {clinical_note}\n",
    "\n",
    "            Return the output in the following format:\n",
    "\n",
    "            [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", ..., \"ICD10_CODE_5\"]\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Generate top 5 ICD-10 codes with descriptions\n",
    "    # message = [\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"\"\"\n",
    "    #         You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "    #         Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes along with their descriptions. No explanation or additional information is required in the output.\n",
    "\n",
    "    #         Clinical Note:\n",
    "    #         {clinical_note}\n",
    "\n",
    "    #         Return the output in the following format:\n",
    "    #         [\n",
    "    #             {{\"ICD10_CODE_1\": \"ICD10_CODE_1_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_2\": \"ICD10_CODE_2_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_3\": \"ICD10_CODE_3_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_4\": \"ICD10_CODE_4_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_5\": \"ICD10_CODE_5_DESCRIPTION\"}}\n",
    "    #         ]\n",
    "    #         \"\"\"\n",
    "    #     }\n",
    "    # ]\n",
    "\n",
    "\n",
    "    print(\"\\n[INFO] Sending request to the Hugging Face model Llama-3.1-8B-Instruct.........\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        messages=message,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    print(\"\\n [INFO] completion...\")\n",
    "    print(completion)\n",
    "\n",
    "    icd_response_01 = completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "    print(\"\\n [INFO] response...\")\n",
    "    print(icd_response_01)\n",
    "\n",
    "    print(\"====================================================================================\")\n",
    "    #print(f\"[DEBUG] Clinical Note: {clinical_note[:200]}...\")  # Show first 200 characters\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "meta-llama/Llama-3.2-1B\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace API Configuration\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "HF_API_KEY = \"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\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HF_API_KEY}\", \"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake Configuration\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    \"user\": \"DOLPHIN\",\n",
    "    \"password\": \"Maapaa@1603\",\n",
    "    \"account\": \"URB63596\",\n",
    "    \"warehouse\": \"ANIMAL_TASK_WH\",\n",
    "    \"database\": \"mimic_iv_medi_assist\",\n",
    "    \"schema\": \"staging_mimc\",\n",
    "}\n",
    "DISCHARGE_TABLE = \"MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connecting to Snowflake and fetching a single clinical note...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/v5dnvjg12ws3p1zszn_fvgt80000gn/T/ipykernel_20749/2337959753.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  clinical_notes_df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Clinical note fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fetch a single record from Snowflake\n",
    "print(\"[INFO] Connecting to Snowflake and fetching a single clinical note...\")\n",
    "try:\n",
    "    connection = snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
    "    query = f\"\"\"\n",
    "    SELECT SUBJECT_ID, HADM_ID, TEXT \n",
    "    FROM {DISCHARGE_TABLE}\n",
    "    WHERE TEXT IS NOT NULL\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    clinical_notes_df = pd.read_sql(query, connection)\n",
    "    connection.close()\n",
    "    print(\"[INFO] Clinical note fetched successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to fetch data from Snowflake: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[INFO] Processing SUBJECT_ID: 15992303, HADM_ID: 22502053\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process the single clinical note\n",
    "if clinical_notes_df.empty:\n",
    "    print(\"[INFO] No clinical note available to process.\")\n",
    "    exit(0)\n",
    "\n",
    "row = clinical_notes_df.iloc[0]\n",
    "subject_id = row[\"SUBJECT_ID\"]\n",
    "hadm_id = row[\"HADM_ID\"]\n",
    "clinical_note = row[\"TEXT\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"[INFO] Processing SUBJECT_ID: {subject_id}, HADM_ID: {hadm_id}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the instance payload\n",
    "payload = {\n",
    "    \"inputs\": f\"\"\"\n",
    "    You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "    Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes along with their descriptions. No explanation or additional information is required in the output.\n",
    "\n",
    "    Clinical Note:\n",
    "    {clinical_note}\n",
    "\n",
    "    Return the output in the following format:\n",
    "    [\n",
    "        {{\"ICD10_CODE_1\": \"ICD10_CODE_1_DESCRIPTION\"}},\n",
    "        {{\"ICD10_CODE_2\": \"ICD10_CODE_2_DESCRIPTION\"}},\n",
    "        {{\"ICD10_CODE_3\": \"ICD10_CODE_3_DESCRIPTION\"}},\n",
    "        {{\"ICD10_CODE_4\": \"ICD10_CODE_4_DESCRIPTION\"}},\n",
    "        {{\"ICD10_CODE_5\": \"ICD10_CODE_5_DESCRIPTION\"}}\n",
    "    ]\n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sending request to HuggingFace API...\n",
      "[ERROR] HuggingFace API failed with status 422: {\"error\":\"Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 5221 `inputs` tokens and 100 `max_new_tokens`\",\"error_type\":\"validation\"}\n",
      "[INFO] Workflow completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Query the HuggingFace model\n",
    "try:\n",
    "    print(\"[INFO] Sending request to HuggingFace API...\")\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        icd_codes = response.json()\n",
    "        print(\"[INFO] Top 5 ICD Codes with Descriptions:\")\n",
    "        print(json.dumps(icd_codes, indent=2))\n",
    "    else:\n",
    "        print(f\"[ERROR] HuggingFace API failed with status {response.status_code}: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to query HuggingFace API for SUBJECT_ID {subject_id}, HADM_ID {hadm_id}: {e}\")\n",
    "\n",
    "print(\"[INFO] Workflow completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "meta-llama/Llama-3.2-1B-Instruct\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Initializing Hugging Face client...\n",
      "\n",
      "====================================================================================================\n",
      "RESPONSE\n",
      "====================================================================================================\n",
      "ICD-10 Codes: [\"T19.8901\", \"E03.5\", \"A16.01\", \"T70.3\", \"E08.0\"]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "# Hugging Face API Key\n",
    "HF_API_KEY = \"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\"\n",
    "\n",
    "# Initialize Hugging Face Inference Client\n",
    "print(\"\\n[INFO] Initializing Hugging Face client...\\n\")\n",
    "client = InferenceClient(api_key=HF_API_KEY)\n",
    "\n",
    "# Define the clinical note and message\n",
    "clinical_note = \"\"\"\n",
    "Patient presented with severe abdominal pain and elevated bilirubin levels. Imaging revealed gallstones obstructing the bile duct.\n",
    "\"\"\"\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": f\"\"\"\n",
    "#         You are an advanced clinical language model with expertise in interpreting complex medical information and mapping it to precise ICD-10 codes. \n",
    "#         Below is a detailed patient's clinical note. Your task is to generate a Python list containing ONLY the top 5 most relevant ICD-10 codes associated with the clinical note below.\n",
    "#         Follow the expected Output Format and Output Requirements mentioned below.\n",
    "\n",
    "#         Output Requirements:\n",
    "#         - Provide **only** the ICD-10 codes in the form of a Python list.\n",
    "#         - **Do not** include any explanations, descriptions, or additional context.\n",
    "#         - No explanation in the output.\n",
    "\n",
    "#         Clinical Note:\n",
    "#         {clinical_note}\n",
    "\n",
    "#         Expected Output Format:\n",
    "#         [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", \"ICD10_CODE_4\", \"ICD10_CODE_5\"]\n",
    "#         \"\"\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        You are an advanced clinical language model with expertise in ICD-10 coding. \n",
    "        Generate ONLY the top 5 most important ICD-10 codes associated with the clinical note below.\n",
    "\n",
    "        Output Requirements:\n",
    "        - Provide **only** the ICD-10 codes in a plain Python list.\n",
    "        - **Do not** include any explanations, descriptions, or additional text.\n",
    "        - Ensure the codes are formatted as follows:\n",
    "        \n",
    "        [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", \"ICD10_CODE_4\", \"ICD10_CODE_5\"]\n",
    "\n",
    "        Clinical Note:\n",
    "        {clinical_note}\n",
    "\n",
    "        Return your output strictly in the above format with no additional text.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Send the request to the model\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "    messages=messages, \n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"RESPONSE\")\n",
    "print(\"=\"*100)\n",
    "# Extract and print the response\n",
    "response = completion.choices[0].message[\"content\"]\n",
    "print(\"ICD-10 Codes:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Initializing Hugging Face client...\n",
      "\n",
      "\n",
      "[INFO] Step 2: Processing clinical notes...\n",
      "====================================================================================\n",
      "\n",
      "[INFO] Processing SUBJECT_ID: 15992303 \n",
      "[INFO] Processing HADM_ID: 22502053\n",
      "\n",
      "[INFO] Formatting message for the model...\n",
      "\n",
      "[INFO] Sending request to the Hugging Face model Llama-3.2-1B-Instruct.........\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B-Instruct/v1/chat/completions (Request ID: Tg1ZUw2MENIG74E7JP72m)\n\nInput validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 5192 `inputs` tokens and 500 `max_new_tokens`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 88\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# message = [\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#     {\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#         \"role\": \"user\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] Sending request to the Hugging Face model Llama-3.2-1B-Instruct.........\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# print(\"\\n [INFO] completion...\")\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# print(completion)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# print(\"====================================================================================\")\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#print(f\"[DEBUG] Clinical Note: {clinical_note[:200]}...\")  # Show first 200 characters\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/inference/_client.py:882\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    860\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    861\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    862\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    879\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m payload \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 882\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/inference/_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B-Instruct/v1/chat/completions (Request ID: Tg1ZUw2MENIG74E7JP72m)\n\nInput validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 5192 `inputs` tokens and 500 `max_new_tokens`"
     ]
    }
   ],
   "source": [
    "#HF_API_KEY = \"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\"\n",
    "HF_API_KEY = \"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\"\n",
    "\n",
    "# Initialize the Hugging Face Inference Client\n",
    "print(\"\\n[INFO] Initializing Hugging Face client...\\n\")\n",
    "client = InferenceClient(api_key=HF_API_KEY)\n",
    "\n",
    "print(\"\\n[INFO] Step 2: Processing clinical notes...\")\n",
    "\n",
    "for _, row in clinical_notes_df.iterrows():\n",
    "    print(\"====================================================================================\")\n",
    "    subject_id = row[\"SUBJECT_ID\"]\n",
    "    hadm_id = row[\"HADM_ID\"]\n",
    "    clinical_note = row[\"TEXT\"]\n",
    "\n",
    "    print(f\"\\n[INFO] Processing SUBJECT_ID: {subject_id} \\n[INFO] Processing HADM_ID: {hadm_id}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Formatting message for the model...\")\n",
    "\n",
    "    #TOP 10 ICD CODES\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are an advanced clinical language model with expertise in ICD-10 coding. \n",
    "            Generate ONLY the top 2 most important ICD-10 codes associated with the clinical note below.\n",
    "\n",
    "            Output Requirements:\n",
    "            - Provide **only** the ICD-10 codes in a plain Python list.\n",
    "            - **Do not** include any explanations, descriptions, or additional text.\n",
    "            - Ensure the codes are formatted as follows:\n",
    "            \n",
    "            [\"ICD10_CODE_1\", \"ICD10_CODE_2\"]\n",
    "\n",
    "            Clinical Note:\n",
    "            {clinical_note}\n",
    "\n",
    "            Return your output strictly in the above format with no additional text.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # message = [\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"\"\"\n",
    "    #         You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "    #         Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes as a Python list of strings. No explanation and code required in the output.\n",
    "\n",
    "    #         Clinical Note:\n",
    "    #         {clinical_note}\n",
    "\n",
    "    #         Return the output in the following format:\n",
    "\n",
    "    #         [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", ..., \"ICD10_CODE_5\"]\n",
    "    #         \"\"\"\n",
    "    #     }\n",
    "    # ]\n",
    "\n",
    "    # Generate top 5 ICD-10 codes with descriptions\n",
    "    # message = [\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"\"\"\n",
    "    #         You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "    #         Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes along with their descriptions. No explanation or additional information is required in the output.\n",
    "\n",
    "    #         Clinical Note:\n",
    "    #         {clinical_note}\n",
    "\n",
    "    #         Return the output in the following format:\n",
    "    #         [\n",
    "    #             {{\"ICD10_CODE_1\": \"ICD10_CODE_1_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_2\": \"ICD10_CODE_2_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_3\": \"ICD10_CODE_3_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_4\": \"ICD10_CODE_4_DESCRIPTION\"}},\n",
    "    #             {{\"ICD10_CODE_5\": \"ICD10_CODE_5_DESCRIPTION\"}}\n",
    "    #         ]\n",
    "    #         \"\"\"\n",
    "    #     }\n",
    "    # ]\n",
    "\n",
    "\n",
    "    print(\"\\n[INFO] Sending request to the Hugging Face model Llama-3.2-1B-Instruct.........\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "        messages=message,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message)\n",
    "    # print(\"\\n [INFO] completion...\")\n",
    "    # print(completion)\n",
    "\n",
    "    # icd_response_01 = completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "    # print(\"\\n [INFO] response...\")\n",
    "    # print(icd_response_01)\n",
    "\n",
    "    # print(\"====================================================================================\")\n",
    "    #print(f\"[DEBUG] Clinical Note: {clinical_note[:200]}...\")  # Show first 200 characters\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Initializing Hugging Face client...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HF_API_KEY = \"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\"\n",
    "\n",
    "# Initialize the Hugging Face Inference Client\n",
    "print(\"\\n[INFO] Initializing Hugging Face client...\\n\")\n",
    "client = InferenceClient(api_key=HF_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Step 2: Processing clinical notes...\n",
      "====================================================================================\n",
      "\n",
      "[INFO] Processing SUBJECT_ID: 15992303 \n",
      "[INFO] Processing HADM_ID: 22502053\n",
      "\n",
      "[INFO] Formatting message for the model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] Step 2: Processing clinical notes...\")\n",
    "\n",
    "for _, row in clinical_notes_df.iterrows():\n",
    "    print(\"====================================================================================\")\n",
    "    subject_id = row[\"SUBJECT_ID\"]\n",
    "    hadm_id = row[\"HADM_ID\"]\n",
    "    clinical_note = row[\"TEXT\"]\n",
    "\n",
    "    print(f\"\\n[INFO] Processing SUBJECT_ID: {subject_id} \\n[INFO] Processing HADM_ID: {hadm_id}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Formatting message for the model...\")\n",
    "\n",
    "    #TOP 10 ICD CODES\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "            Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes as a Python list of strings. No explanation and code required in the output.\n",
    "\n",
    "            Clinical Note:\n",
    "            {clinical_note}\n",
    "\n",
    "            Return the output in the following format:\n",
    "\n",
    "            [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", ..., \"ICD10_CODE_5\"]\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B-Instruct/v1/chat/completions (Request ID: XUPnRo82UP5CC-OuCAXvz)\n\nInput validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 5192 `inputs` tokens and 500 `max_new_tokens`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m [INFO] completion...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion)\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/inference/_client.py:882\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    860\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    861\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    862\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    879\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m payload \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 882\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/inference/_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Desktop/Start-to-Complete/MediAssist-Healthcare-Data-Solutions/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B-Instruct/v1/chat/completions (Request ID: XUPnRo82UP5CC-OuCAXvz)\n\nInput validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 5192 `inputs` tokens and 500 `max_new_tokens`"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n [INFO] completion...\")\n",
    "print(completion)\n",
    "\n",
    "icd_response = completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "print(\"\\n [INFO] response...\")\n",
    "print(icd_response)\n",
    "\n",
    "print(\"====================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "GEMMA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from huggingface_hub import InferenceClient\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_USER = \"DOLPHIN\"\n",
    "SNOWFLAKE_PASSWORD = \"Maapaa@1603\"\n",
    "SNOWFLAKE_ACCOUNT = \"URB63596\"\n",
    "SNOWFLAKE_WAREHOUSE = \"ANIMAL_TASK_WH\"\n",
    "SNOWFLAKE_DATABASE = \"mimic_iv_medi_assist\"\n",
    "SNOWFLAKE_SCHEMA = \"staging_mimc\"\n",
    "DISCHARGE_TABLE = \"MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, TEXT \n",
    "FROM MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\n",
    "WHERE TEXT IS NOT NULL\n",
    "LIMIT 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[INFO] Connecting to Snowflake...\")\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    "    warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "    database=SNOWFLAKE_DATABASE,\n",
    "    schema=SNOWFLAKE_SCHEMA,)\n",
    "\n",
    "print(\"[INFO] Executing query to fetch clinical notes...\\n\")\n",
    "clinical_notes_df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n[INFO] printing dataframe data...\\n\")\n",
    "print(clinical_notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HF_API_KEY = \"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\"\n",
    "HF_API_KEY = \"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\"\n",
    "\n",
    "# Initialize the Hugging Face Inference Client\n",
    "print(\"\\n[INFO] Initializing Hugging Face client...\\n\")\n",
    "client = InferenceClient(api_key=HF_API_KEY)\n",
    "\n",
    "print(\"\\n[INFO] Step 2: Processing clinical notes...\")\n",
    "\n",
    "for _, row in clinical_notes_df.iterrows():\n",
    "    print(\"====================================================================================\")\n",
    "    subject_id = row[\"SUBJECT_ID\"]\n",
    "    hadm_id = row[\"HADM_ID\"]\n",
    "    clinical_note = row[\"TEXT\"]\n",
    "\n",
    "    print(f\"\\n[INFO] Processing SUBJECT_ID: {subject_id} \\n[INFO] Processing HADM_ID: {hadm_id}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Formatting message for the model...\")\n",
    "\n",
    "    #TOP 10 ICD CODES\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "            Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes as a Python list of strings. No explanation and code required in the output.\n",
    "\n",
    "            Clinical Note:\n",
    "            {clinical_note}\n",
    "\n",
    "            Return the output in the following format:\n",
    "\n",
    "            [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", ..., \"ICD10_CODE_5\"]\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    \n",
    "    print(\"\\n[INFO] Sending request to the Hugging Face model Llama-3.1-8B-Instruct.........\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        messages=message,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    print(\"\\n [INFO] completion...\")\n",
    "    print(completion)\n",
    "\n",
    "    icd_response_01 = completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "    print(\"\\n [INFO] response...\")\n",
    "    print(icd_response_01)\n",
    "\n",
    "    print(\"====================================================================================\")\n",
    "    #print(f\"[DEBUG] Clinical Note: {clinical_note[:200]}...\")  # Show first 200 characters\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-2-2b\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connecting to Snowflake...\n",
      "[INFO] Executing query to fetch clinical notes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/v5dnvjg12ws3p1zszn_fvgt80000gn/T/ipykernel_20749/2682364239.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  clinical_notes_df = pd.read_sql(QUERY, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Clinical notes fetched successfully!\n",
      "   SUBJECT_ID   HADM_ID                                               TEXT\n",
      "0    15992303  22502053   \\nName:  ___               Unit No:   ___\\n \\...\n",
      "================================================================================\n",
      "[INFO] Processing SUBJECT_ID: 15992303, HADM_ID: 22502053\n",
      "[INFO] Formatting input payload...\n",
      "[INFO] Sending request to Hugging Face model...\n",
      "[INFO] Sending request to Hugging Face API...\n",
      "[ERROR] HTTP error occurred: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-2b\n",
      "[ERROR] No response received from the model.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Snowflake Connection Parameters\n",
    "SNOWFLAKE_USER = \"DOLPHIN\"\n",
    "SNOWFLAKE_PASSWORD = \"Maapaa@1603\"\n",
    "SNOWFLAKE_ACCOUNT = \"URB63596\"\n",
    "SNOWFLAKE_WAREHOUSE = \"ANIMAL_TASK_WH\"\n",
    "SNOWFLAKE_DATABASE = \"mimic_iv_medi_assist\"\n",
    "SNOWFLAKE_SCHEMA = \"staging_mimc\"\n",
    "DISCHARGE_TABLE = \"MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\"\n",
    "\n",
    "# Hugging Face API Key and Model\n",
    "HF_API_KEY = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-2-2b\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
    "\n",
    "# Query to fetch clinical notes from Snowflake\n",
    "QUERY = \"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, TEXT \n",
    "FROM MIMIC_IV_MEDI_ASSIST.STAGING_MIMIC.STG_DISCHARGE\n",
    "WHERE TEXT IS NOT NULL\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Connect to Snowflake and fetch clinical notes\n",
    "print(\"[INFO] Connecting to Snowflake...\")\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    "    warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "    database=SNOWFLAKE_DATABASE,\n",
    "    schema=SNOWFLAKE_SCHEMA\n",
    ")\n",
    "\n",
    "print(\"[INFO] Executing query to fetch clinical notes...\")\n",
    "clinical_notes_df = pd.read_sql(QUERY, conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"[INFO] Clinical notes fetched successfully!\")\n",
    "print(clinical_notes_df)\n",
    "\n",
    "# Step 2: Define function to query Hugging Face API\n",
    "def query_huggingface(payload):\n",
    "    \"\"\"\n",
    "    Send request to Hugging Face Inference API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"[INFO] Sending request to Hugging Face API...\")\n",
    "        response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"[ERROR] HTTP error occurred: {http_err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Other error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 3: Process each clinical note\n",
    "for _, row in clinical_notes_df.iterrows():\n",
    "    print(\"=\" * 80)\n",
    "    subject_id = row[\"SUBJECT_ID\"]\n",
    "    hadm_id = row[\"HADM_ID\"]\n",
    "    clinical_note = row[\"TEXT\"]\n",
    "\n",
    "    print(f\"[INFO] Processing SUBJECT_ID: {subject_id}, HADM_ID: {hadm_id}\")\n",
    "    \n",
    "    # Format the input message for the model\n",
    "    print(\"[INFO] Formatting input payload...\")\n",
    "    payload = {\n",
    "        \"inputs\": f\"\"\"\n",
    "        You are an intelligent clinical language model specialized in generating ICD-10 codes.\n",
    "        Below is a patient's clinical note. Generate only the 5 most relevant ICD-10 codes as a Python list of strings. No explanation required in the output.\n",
    "\n",
    "        Clinical Note:\n",
    "        {clinical_note}\n",
    "\n",
    "        Return the output in the following format:\n",
    "        [\"ICD10_CODE_1\", \"ICD10_CODE_2\", \"ICD10_CODE_3\", \"ICD10_CODE_4\", \"ICD10_CODE_5\"]\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # Send the request to the Hugging Face model\n",
    "    print(\"[INFO] Sending request to Hugging Face model...\")\n",
    "    response = query_huggingface(payload)\n",
    "\n",
    "    # Process the response\n",
    "    if response:\n",
    "        print(\"[INFO] Model Response:\")\n",
    "        print(response)\n",
    "    else:\n",
    "        print(\"[ERROR] No response received from the model.\")\n",
    "\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
